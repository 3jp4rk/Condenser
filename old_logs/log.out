[2024-03-18 06:41:08,292] torch.distributed.run: [WARNING] 
[2024-03-18 06:41:08,292] torch.distributed.run: [WARNING] *****************************************
[2024-03-18 06:41:08,292] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-18 06:41:08,292] torch.distributed.run: [WARNING] *****************************************
03/18/2024 06:41:11 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
03/18/2024 06:41:11 - INFO - __main__ -   Training/evaluation parameters CondenserPreTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=32,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./logs/runs/Mar18_06-41-10_ab6613b10048,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=8.0,
optim=adamw_torch,
optim_args=None,
output_dir=./logs,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=./logs,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=20000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.01,
)
03/18/2024 06:41:11 - WARNING - __main__ -   Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:728] 2024-03-18 06:41:11,453 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:11,455 >> Model config BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

03/18/2024 06:41:11 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
03/18/2024 06:41:11 - WARNING - __main__ -   Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True
[INFO|configuration_utils.py:728] 2024-03-18 06:41:11,674 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:11,677 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:11,680 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:11,680 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:11,680 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:11,681 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:11,681 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:11,681 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:11,683 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|modeling_utils.py:3257] 2024-03-18 06:41:11,767 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors
[INFO|configuration_utils.py:845] 2024-03-18 06:41:11,775 >> Generate config GenerationConfig {
  "pad_token_id": 0
}

[WARNING|modeling_utils.py:3982] 2024-03-18 06:41:12,131 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:4000] 2024-03-18 06:41:12,131 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
[INFO|modeling_utils.py:3544] 2024-03-18 06:41:12,361 >> Generation config file not found, using a generation config created from the model config.
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|configuration_utils.py:728] 2024-03-18 06:41:12,606 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:12,608 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|modeling_utils.py:3257] 2024-03-18 06:41:12,661 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/pytorch_model.bin
/root/con_venv/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/root/con_venv/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/root/con_venv/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/root/con_venv/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[INFO|modeling_utils.py:3982] 2024-03-18 06:41:13,102 >> Some weights of the model checkpoint at tunib/electra-ko-en-base were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']
- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:4000] 2024-03-18 06:41:13,103 >> All the weights of ElectraModel were initialized from the model checkpoint at tunib/electra-ko-en-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.
[INFO|modeling_utils.py:1875] 2024-03-18 06:41:13,154 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 61790. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[111474, 111473, 111473, 83605]
[642411, 642409, 642405, 642407, 642410, 642409, 642405, 642413, 642413, 642414, 642416, 642416, 642421, 642423, 642427, 642422, 642419, 642418, 642414, 642418]
txt data : 176283, en txt data : 168329, ko txt data : 7954
418025
12848290
13274269
total ko data : 13274269, current en data : 168329, (ko - current en) data : 13105940
when 90% en data, cul en size should be 119300092
when 80% en data, cul en size should be 52928747
when 70% en data, cul en size should be 30804965
when 60% en data, cul en size should be 19743074
when 50% en data, cul en size should be 13105940
/root/con_venv/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
03/18/2024 06:41:14 - WARNING - accelerate.utils.other -   Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[111474, 111473, 111473, 83605]
[642411, 642409, 642405, 642407, 642410, 642409, 642405, 642413, 642413, 642414, 642416, 642416, 642421, 642423, 642427, 642422, 642419, 642418, 642414, 642418]
txt data : 176283, en txt data : 168329, ko txt data : 7954
418025
12848290
13274269
total ko data : 13274269, current en data : 168329, (ko - current en) data : 13105940
when 90% en data, cul en size should be 119300092
when 80% en data, cul en size should be 52928747
when 70% en data, cul en size should be 30804965
when 60% en data, cul en size should be 19743074
when 50% en data, cul en size should be 13105940
/root/con_venv/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[111474, 111473, 111473, 83605]
[642411, 642409, 642405, 642407, 642410, 642409, 642405, 642413, 642413, 642414, 642416, 642416, 642421, 642423, 642427, 642422, 642419, 642418, 642414, 642418]
txt data : 176283, en txt data : 168329, ko txt data : 7954
418025
12848290
13274269
total ko data : 13274269, current en data : 168329, (ko - current en) data : 13105940
when 90% en data, cul en size should be 119300092
when 80% en data, cul en size should be 52928747
when 70% en data, cul en size should be 30804965
when 60% en data, cul en size should be 19743074
when 50% en data, cul en size should be 13105940
/root/con_venv/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[111474, 111473, 111473, 83605]
[642411, 642409, 642405, 642407, 642410, 642409, 642405, 642413, 642413, 642414, 642416, 642416, 642421, 642423, 642427, 642422, 642419, 642418, 642414, 642418]
txt data : 176283, en txt data : 168329, ko txt data : 7954
418025
12848290
13274269
total ko data : 13274269, current en data : 168329, (ko - current en) data : 13105940
when 90% en data, cul en size should be 119300092
when 80% en data, cul en size should be 52928747
when 70% en data, cul en size should be 30804965
when 60% en data, cul en size should be 19743074
when 50% en data, cul en size should be 13105940
/root/con_venv/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:601] 2024-03-18 06:41:15,461 >> Using auto half precision backend
/root/con_venv/lib/python3.9/site-packages/transformers/trainer.py:1567: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.
  warnings.warn(
/root/con_venv/lib/python3.9/site-packages/transformers/trainer.py:1567: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.
  warnings.warn(
/root/con_venv/lib/python3.9/site-packages/transformers/trainer.py:1567: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.
  warnings.warn(
/root/con_venv/lib/python3.9/site-packages/transformers/trainer.py:1567: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.
  warnings.warn(
[INFO|trainer.py:1812] 2024-03-18 06:41:17,037 >> ***** Running training *****
[INFO|trainer.py:1813] 2024-03-18 06:41:17,038 >>   Num examples = 13,274,273
[INFO|trainer.py:1814] 2024-03-18 06:41:17,038 >>   Num Epochs = 8
[INFO|trainer.py:1815] 2024-03-18 06:41:17,038 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1818] 2024-03-18 06:41:17,038 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:1819] 2024-03-18 06:41:17,038 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1820] 2024-03-18 06:41:17,038 >>   Total optimization steps = 829,640
[INFO|trainer.py:1821] 2024-03-18 06:41:17,039 >>   Number of trainable parameters = 147,735,134
[INFO|integration_utils.py:722] 2024-03-18 06:41:17,180 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: 3jp4rk. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /root/wandb/run-20240318_064118-loax5ril
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-totem-9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/3jp4rk/condenser-bert-pretrain
wandb: üöÄ View run at https://wandb.ai/3jp4rk/condenser-bert-pretrain/runs/loax5ril
  0%|                                                                                          | 0/829640 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (914 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1085 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (928 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1358 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (944 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (937 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3950 > 512). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2488 > 512). Running this sequence through the model will result in indexing errors
Traceback (most recent call last):
  File "/root/Condenser/run_pre_training.py", line 219, in <module>
    main()
  File "/root/Condenser/run_pre_training.py", line 189, in main
    trainer.train(model_path=model_path)
  File "/root/con_venv/lib/python3.9/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
  File "/root/con_venv/lib/python3.9/site-packages/transformers/trainer.py", line 1928, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/root/con_venv/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/root/con_venv/lib/python3.9/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
AttributeError: Caught AttributeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/root/Condenser/data.py", line 148, in __call__
    inputs, labels = self.mask_tokens(
AttributeError: 'CondenserCollator' object has no attribute 'mask_tokens'

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,874 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,888 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,890 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,891 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,892 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,892 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,892 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,892 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,892 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,893 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,901 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,905 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,906 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,906 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,906 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,907 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,907 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,907 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,907 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,907 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,907 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,907 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,907 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,908 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,908 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,908 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,908 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,908 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,908 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,908 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,908 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,908 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,908 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,908 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,908 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,908 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,908 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,909 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,908 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,909 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,909 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,909 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,909 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,909 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,909 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,909 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,909 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,909 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,909 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,909 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,909 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,909 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,910 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,910 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,910 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,910 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,910 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,910 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,910 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,910 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,907 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,907 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,908 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,908 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,908 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,911 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,911 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,911 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,911 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,911 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,911 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,911 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,912 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,912 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,912 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,912 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,912 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,913 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,913 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,913 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,913 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,913 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,913 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,913 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,913 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,913 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,913 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,913 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,913 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,914 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,914 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,914 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,914 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,915 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,915 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,915 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,915 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,915 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,915 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,915 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,917 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,917 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,918 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,919 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,919 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,926 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,926 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,927 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,927 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,927 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,927 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,928 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,928 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,928 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,931 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,931 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,931 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,931 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,931 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,932 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,933 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,936 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,939 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,939 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,939 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,939 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,939 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,940 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,941 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,919 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,919 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,919 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,919 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,920 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,920 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,921 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,922 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,924 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,951 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,951 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,951 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,951 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,952 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,952 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,952 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,952 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,952 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,952 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,952 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,925 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,925 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,952 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,952 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,926 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,953 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,927 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,953 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,928 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,951 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,929 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,953 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,953 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,953 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,953 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,953 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,953 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,954 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,954 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,954 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,954 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,954 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,954 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,954 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,954 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,954 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,954 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,955 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,955 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,955 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,955 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,955 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,955 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,955 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,955 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,956 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,956 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,956 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,956 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,956 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,956 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,956 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,956 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,956 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,956 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,956 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,956 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,957 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,957 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,957 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,957 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,957 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,957 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,957 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,957 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,957 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,957 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,957 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,957 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,956 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,958 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,958 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,958 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,958 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,958 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,958 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,958 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,958 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,959 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,960 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,960 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,961 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,961 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,961 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,961 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,961 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,961 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,961 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,961 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,961 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,962 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,962 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,963 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,963 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,966 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,968 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,969 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,969 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,970 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,970 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,970 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,970 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,971 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,971 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,971 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,972 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,972 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:23,972 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,972 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,972 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,973 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:23,974 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:23,982 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,014 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,083 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,083 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,085 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,087 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,087 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,088 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,089 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,089 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,089 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,089 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,089 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,090 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,092 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,095 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,095 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,096 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,099 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,112 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,114 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,125 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,127 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,129 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,130 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,150 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,152 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,161 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,163 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,180 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,181 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,190 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,192 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,227 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,228 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,242 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,243 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,266 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,268 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,294 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,297 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,298 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,298 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,298 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,298 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,299 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,299 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,300 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,315 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,318 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

Traceback (most recent call last):
  File "/root/Condenser/run_pre_training.py", line 219, in <module>
    main()
  File "/root/Condenser/run_pre_training.py", line 189, in main
    trainer.train(model_path=model_path)
  File "/root/con_venv/lib/python3.9/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
  File "/root/con_venv/lib/python3.9/site-packages/transformers/trainer.py", line 1928, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/root/con_venv/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/root/con_venv/lib/python3.9/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
AttributeError: Caught AttributeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/root/Condenser/data.py", line 148, in __call__
    inputs, labels = self.mask_tokens(
AttributeError: 'CondenserCollator' object has no attribute 'mask_tokens'

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,352 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,355 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,397 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,399 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,399 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,399 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,400 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,399 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,400 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,400 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,400 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,401 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,402 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,404 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,406 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,407 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,408 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,408 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,408 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,408 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,409 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,409 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,409 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,410 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,410 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,412 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,425 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,426 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,479 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,480 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,481 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,481 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,481 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,482 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,482 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,482 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,483 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,488 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,489 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,493 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,494 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,500 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,503 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,504 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,504 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,504 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,504 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,505 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,505 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,507 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,510 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,512 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,513 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,513 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,515 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,515 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,516 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,517 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,517 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,517 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,517 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,517 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,517 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,517 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:24,517 >> Token indices sequence length is longer than the specified maximum sequence length for this model (740 > 512). Running this sequence through the model will result in indexing errors
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,517 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,518 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,518 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,519 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,520 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:24,520 >> Token indices sequence length is longer than the specified maximum sequence length for this model (775 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,520 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,534 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,536 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,536 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,537 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,537 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,537 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,537 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,537 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,538 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,538 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,538 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,539 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,539 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,539 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,539 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,539 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,539 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,540 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,540 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,541 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,548 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,549 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,566 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,568 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,575 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,576 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,578 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,579 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,579 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,580 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,590 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,591 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,592 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,593 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,593 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,595 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,598 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,599 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,620 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,622 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,623 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,623 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,623 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,623 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,623 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,624 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,625 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,633 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,635 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,649 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,651 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,652 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
Token indices sequence length is longer than the specified maximum sequence length for this model (707 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,653 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,653 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,654 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,654 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,654 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,654 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,654 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,654 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,655 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,655 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,655 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,655 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,655 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,655 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,655 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,655 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,656 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,658 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,659 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,660 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,662 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,663 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,664 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,714 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,715 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,716 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,716 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,717 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,717 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,717 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,718 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,718 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,718 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,720 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,720 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,721 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,721 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,722 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,722 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,722 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,722 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,722 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,723 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,723 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,724 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,742 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,743 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,760 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,760 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,762 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,762 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,762 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,763 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,763 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,763 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,763 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,763 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,763 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,763 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,763 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,763 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,763 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,763 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,764 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,764 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,765 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,765 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,766 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,768 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,769 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,769 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,769 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,769 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,769 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,770 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,772 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:24,788 >> Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,800 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,802 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,803 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,803 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,803 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,804 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,804 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,804 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,806 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,811 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,812 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,813 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,813 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,814 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,814 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,814 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,814 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,814 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,814 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,815 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,815 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,815 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,815 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,815 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,815 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,816 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,817 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,823 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,824 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,833 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,834 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,835 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,835 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,838 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,838 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,838 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,839 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,839 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,840 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,840 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,841 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,841 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,841 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,841 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,841 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,843 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,843 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,844 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,845 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,845 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,845 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,846 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,845 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,846 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,846 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,846 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,846 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,846 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,846 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,846 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,846 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,846 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,846 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,847 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,847 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,848 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,849 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,849 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,849 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,849 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,849 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,850 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,851 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

Traceback (most recent call last):
  File "/root/Condenser/run_pre_training.py", line 219, in <module>
    main()
  File "/root/Condenser/run_pre_training.py", line 189, in main
    trainer.train(model_path=model_path)
  File "/root/con_venv/lib/python3.9/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
  File "/root/con_venv/lib/python3.9/site-packages/transformers/trainer.py", line 1928, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/root/con_venv/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/root/con_venv/lib/python3.9/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
AttributeError: Caught AttributeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/root/Condenser/data.py", line 148, in __call__
    inputs, labels = self.mask_tokens(
AttributeError: 'CondenserCollator' object has no attribute 'mask_tokens'

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,865 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,867 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,868 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,868 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,868 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,868 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,868 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,869 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,870 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,880 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,885 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,900 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,904 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,904 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,904 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,904 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,904 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,904 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,904 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,905 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,906 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,906 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,906 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,906 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,906 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,906 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,907 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,908 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,909 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,910 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,910 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,911 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,911 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,913 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,914 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,914 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,916 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,916 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,916 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,916 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,916 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,916 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,916 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,917 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,917 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,917 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,917 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,918 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,918 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,918 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,918 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,919 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,921 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,922 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,923 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,923 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,923 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,923 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,923 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,924 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,925 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,930 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,932 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,948 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,949 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,950 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,950 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,950 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,951 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,951 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,951 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,952 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,952 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,952 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,952 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,952 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,952 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,953 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,953 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,953 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,953 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,953 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,953 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,953 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,953 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,953 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,953 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,953 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,953 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,954 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,954 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,954 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,959 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,961 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,962 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,962 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,962 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,962 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,962 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,963 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,965 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,978 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,981 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,982 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,982 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,982 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,982 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,983 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,983 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,983 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,983 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,984 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,985 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,985 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,986 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,986 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,986 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,986 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,986 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,986 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,986 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,986 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,986 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,987 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,987 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,987 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,987 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,987 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,987 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,989 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,989 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,990 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,991 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,991 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,991 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,991 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:24,991 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:24,992 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:24,993 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,000 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,001 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,004 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,005 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,007 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,008 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,009 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,009 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,009 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,009 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,009 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,010 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,010 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,016 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,017 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,022 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,023 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,037 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,039 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,040 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,040 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,041 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,041 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,041 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,041 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,043 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,045 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,046 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,055 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,057 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,058 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,058 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,058 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,058 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,058 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,059 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,060 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,067 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,069 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,078 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,079 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,079 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,079 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,080 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,081 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,083 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,084 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,086 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,087 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,091 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,092 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,094 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,095 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,104 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,105 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,106 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,106 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,109 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,111 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,112 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,112 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,113 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,113 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,113 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,113 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,115 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,116 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,118 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,119 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,119 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,119 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,119 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,119 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,120 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,121 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,126 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,127 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,128 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,129 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,134 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,135 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,136 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,138 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,138 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,139 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,139 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,139 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,139 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,139 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,141 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,158 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,160 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,162 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,162 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,162 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,162 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,162 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,163 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,164 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,183 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,185 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,186 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,186 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,186 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,186 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,186 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,186 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,186 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,187 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,188 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,195 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,196 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,197 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,197 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,198 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,198 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,198 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,198 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,198 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,198 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,199 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,201 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,202 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,203 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,204 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,204 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,204 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,205 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,205 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,205 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,205 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,205 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,205 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,205 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,205 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,205 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,206 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,207 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,207 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,232 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,234 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,235 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,235 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,235 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,235 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,235 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,236 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,237 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,242 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,242 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,243 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,243 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,256 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,258 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,259 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,259 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,259 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,259 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,259 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,260 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,262 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,268 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,269 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,275 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,275 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,276 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,277 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,277 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,278 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,278 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,278 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,278 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,278 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,278 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,278 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,278 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,278 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,278 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,278 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,278 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,279 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,279 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,279 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,279 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,279 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,279 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,279 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,280 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,280 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,281 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,281 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,282 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,285 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,287 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,287 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,288 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,288 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,288 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,288 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,288 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,289 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,289 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,290 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,290 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,291 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,291 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,291 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,291 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,291 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,291 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,292 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,292 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,292 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,292 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,292 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,292 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,292 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,293 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,293 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,293 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,294 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,294 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,294 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,314 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,315 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,315 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,315 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,315 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,316 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,316 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,316 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,317 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,326 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,327 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,330 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,331 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,335 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,337 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,338 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,338 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,338 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,338 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,338 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,338 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,340 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,346 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,346 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,348 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,348 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,349 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,349 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,349 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,349 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,349 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,349 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,349 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,349 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,349 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,350 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,350 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,350 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,350 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,351 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,351 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,351 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,351 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,352 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,352 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,352 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,352 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,352 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,354 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,354 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,356 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,361 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,362 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,363 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,363 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,363 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,363 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,363 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,364 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,365 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,366 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,366 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,367 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,367 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,368 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,369 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,369 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,370 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,371 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,371 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,372 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,372 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,372 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,372 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,372 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,372 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,372 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,373 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,373 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,373 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,373 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,373 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,374 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,374 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,375 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,375 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,376 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,378 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,378 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,379 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,379 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,379 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,379 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,379 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,379 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,379 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,379 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,381 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,381 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,381 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,381 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,382 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,382 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,382 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,382 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,383 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,385 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,386 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,399 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,401 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,401 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,402 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,402 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,402 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,402 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,402 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,404 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,404 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:25,405 >> Token indices sequence length is longer than the specified maximum sequence length for this model (834 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,406 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,406 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,406 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,407 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,407 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,407 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,407 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,407 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,407 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,407 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,408 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,408 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,408 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,408 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,408 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,408 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,408 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,408 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,408 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,408 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,408 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,409 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,409 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,409 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,410 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,411 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,420 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,420 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,421 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,422 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,423 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,423 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,423 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,423 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,423 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,424 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,424 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,425 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,425 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,431 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,433 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,437 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,438 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,447 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,448 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,455 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,456 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,467 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,468 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,474 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,475 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,487 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,487 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,488 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,488 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,489 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,489 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,489 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,489 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,489 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,490 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,492 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,496 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,496 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,496 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,497 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,498 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,498 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,498 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,498 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,498 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,499 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,499 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,499 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,499 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,499 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,501 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,502 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,503 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,510 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,511 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,511 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,513 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,514 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,514 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,514 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,514 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,514 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,514 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,516 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,538 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,539 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,540 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,540 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,540 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,540 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,540 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,540 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,541 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,555 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,557 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,558 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,558 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,558 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,558 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,558 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,559 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,560 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,566 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,568 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,568 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,568 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,569 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,569 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,569 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,569 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,569 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,569 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,570 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,570 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,570 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,570 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,570 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,570 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,570 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,570 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,571 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,571 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,583 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,584 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,595 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,596 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,597 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,597 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,598 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,598 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,598 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,598 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,600 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,600 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,601 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,612 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,614 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,615 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,615 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,615 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,615 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,615 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,616 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,617 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,617 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,618 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,622 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,624 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,624 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,624 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,625 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,625 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,625 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,625 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,627 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,635 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,637 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,638 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,638 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,638 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,638 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,638 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,639 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,639 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,640 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,641 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,641 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,641 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,641 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,641 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,641 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,642 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,642 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,643 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,644 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,644 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,645 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,645 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,645 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,645 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,645 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,645 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,646 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,646 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,646 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,646 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,646 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,646 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,646 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,647 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,647 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,648 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,648 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,649 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,649 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,649 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,649 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,649 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,649 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,651 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,653 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,653 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,654 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,654 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,661 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,663 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,663 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,664 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,664 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,664 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,664 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,664 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,664 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,665 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,666 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,666 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,666 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,666 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,666 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,666 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,666 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,668 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,668 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,669 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,678 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,680 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,681 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,681 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,681 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,681 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,681 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,681 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,682 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,682 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,683 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,683 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,683 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,683 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,683 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,683 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,683 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,684 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,688 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,689 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,701 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,703 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,704 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,704 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,704 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,704 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,704 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,705 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,706 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,706 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,707 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,708 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,709 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,710 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,712 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,713 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,713 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,714 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,714 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,714 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,714 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,714 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,715 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,716 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,716 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,717 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,724 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,726 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,727 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,727 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,727 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,728 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,728 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,728 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,728 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,728 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,728 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,728 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,729 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,730 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,730 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,730 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,730 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,730 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,730 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,730 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,730 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,731 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,731 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,731 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,731 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,731 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,731 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,732 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,733 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,741 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,743 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,744 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,744 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,745 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,745 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,745 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,745 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,746 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,747 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,747 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,749 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,750 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,753 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,753 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,754 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,754 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,755 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,755 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,755 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,755 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,755 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,755 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,756 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,757 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,758 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,758 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,759 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,760 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,762 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,763 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,763 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,763 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,763 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,763 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,764 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,765 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,770 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,770 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,772 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,772 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,773 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,773 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,773 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,773 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,773 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,773 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,773 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,773 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,773 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,773 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,773 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,774 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,775 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,775 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,776 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,778 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,779 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,779 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,779 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,779 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,779 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,779 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,781 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,781 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,782 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,795 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,796 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,796 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,796 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,797 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,798 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,799 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,799 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,799 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,799 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,799 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,799 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,801 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,821 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,822 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,824 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,825 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,827 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,828 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,832 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,834 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,834 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,834 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,835 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,835 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,835 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,835 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,835 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,835 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,837 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,837 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,839 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,852 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,852 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,853 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,853 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,855 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,857 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,858 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,858 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,858 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,858 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,858 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,858 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,859 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,860 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,861 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,863 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,864 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,867 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,868 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,879 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,881 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,881 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,882 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,882 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,882 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,882 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,882 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,883 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,883 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,883 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,884 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,884 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,884 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,884 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,884 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,884 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,886 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,908 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,909 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,909 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,911 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,912 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,912 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,912 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,912 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,912 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,912 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,913 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,935 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,936 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,938 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,940 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,940 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,940 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,941 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,941 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,941 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,941 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,941 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,942 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,942 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,943 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,943 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,943 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,943 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,943 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,943 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,945 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,958 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,959 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,959 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,960 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:25,976 >> Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,978 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,980 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,981 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,981 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,981 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,981 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,981 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,982 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,983 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,983 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,985 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,985 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,986 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,986 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,986 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:25,986 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,986 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,987 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:25,990 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:25,991 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,010 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,011 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,013 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,015 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,015 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,015 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,016 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,016 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,016 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,016 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,016 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,016 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,016 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,017 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,017 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,017 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,017 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,017 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,018 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,018 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,018 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,018 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,018 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,018 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,018 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,018 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,018 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,019 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,021 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,021 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,023 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,024 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,024 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,025 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,025 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,025 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,025 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,026 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,027 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,028 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,033 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,035 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,036 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,036 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,036 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,036 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,036 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,036 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,038 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,055 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,056 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,057 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,057 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,057 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,058 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,058 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,058 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,058 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,058 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,059 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,059 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,060 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,060 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,060 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,060 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,061 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,061 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,061 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,063 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,064 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,065 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,070 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,073 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,074 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,074 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,074 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,074 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,074 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,075 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,077 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,090 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,091 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,092 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,092 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,093 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,093 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,093 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,093 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,093 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,094 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,095 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,099 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,100 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,101 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,102 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,103 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,103 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,104 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,104 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,104 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,104 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,104 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,104 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,105 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,106 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,107 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,108 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,108 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,108 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,108 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,108 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,108 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,108 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,109 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,110 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,110 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,110 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,111 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,111 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,111 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,111 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,111 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,111 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,112 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,112 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,112 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,113 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,113 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,113 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,113 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,113 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,113 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,114 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,114 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,114 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,115 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,115 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,115 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,115 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,115 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,115 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,116 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,116 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,117 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,117 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,119 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,120 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,120 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,120 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,120 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,120 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,121 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,122 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,122 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,124 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,125 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,125 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,125 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,125 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,125 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,126 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,126 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,128 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,128 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,129 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,129 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,129 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,129 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,130 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,130 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,132 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,137 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,138 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,139 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,140 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,140 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,140 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,140 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,140 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,140 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,140 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,141 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,141 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,141 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,141 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,141 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,141 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,141 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,142 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,142 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,142 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,142 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,142 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,142 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,142 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,142 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,143 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,143 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,143 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,143 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,151 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,153 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,154 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,154 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,154 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,154 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,154 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,155 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,155 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,156 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,157 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,158 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,158 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,158 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,158 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,158 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,159 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,160 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,166 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,167 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,170 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,172 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,173 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,173 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,173 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,173 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,174 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,174 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,176 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,187 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,189 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,189 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,190 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,190 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,190 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,190 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,190 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,190 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,191 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,198 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,204 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,206 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,207 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,209 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,211 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,212 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,212 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,213 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,213 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,213 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,213 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,214 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,214 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,214 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,215 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,215 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,215 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,215 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,215 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,216 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,216 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,217 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,217 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,217 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,218 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,218 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,229 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,230 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,232 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,234 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,236 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,236 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,236 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,236 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,236 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,237 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,237 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,237 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,238 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,238 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,239 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,239 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,239 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,239 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,240 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,244 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,245 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,246 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,247 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,254 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,255 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,260 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,262 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,270 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,271 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,271 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,273 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,274 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,274 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,274 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,274 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,274 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,275 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,277 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,277 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,279 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,287 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,288 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,288 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,289 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,290 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,292 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,293 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,293 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,293 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,293 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,293 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,294 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,294 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,295 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,295 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,323 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,324 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,324 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,326 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,327 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,327 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,328 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,328 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,328 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,328 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,328 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,329 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,329 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,329 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,329 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,329 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,330 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,330 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,330 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,331 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,355 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,355 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,356 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,357 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,358 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,358 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,358 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,358 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,358 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,358 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,360 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,361 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,362 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,363 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,363 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,363 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,363 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,363 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,364 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,365 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,379 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,380 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,402 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,404 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,405 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,405 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,405 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,405 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,405 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,406 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,407 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,409 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,410 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,411 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,413 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,414 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,414 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,414 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,414 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,414 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,415 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,416 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,419 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,419 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,420 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,421 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,422 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,422 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,422 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,422 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,422 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,423 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,424 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,428 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,429 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,431 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,432 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,433 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,435 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,436 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,436 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,436 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,436 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,436 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,436 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,438 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,460 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,462 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,463 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,463 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,463 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,463 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,463 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,464 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,465 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,465 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,467 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,467 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,467 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,468 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,468 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,468 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,468 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,470 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,480 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,481 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,482 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,482 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,482 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,483 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,483 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,483 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,483 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,483 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,484 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,484 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,484 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,484 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,484 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,485 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,485 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,486 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,492 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,493 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,494 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,494 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,494 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,494 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,494 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,495 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,495 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,496 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,496 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,500 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,501 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,501 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,502 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,503 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,503 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,503 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,503 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,503 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,504 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,504 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,505 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,505 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,507 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,508 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,509 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,509 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,509 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,509 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,509 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,509 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,510 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,512 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,513 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,513 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,513 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,514 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,514 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,514 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,515 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,515 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,515 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,515 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,515 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,515 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,515 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,515 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,516 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,516 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,516 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,516 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,516 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,516 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,516 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,516 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,517 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,517 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,517 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,518 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,518 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,519 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,520 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,520 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,520 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,520 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,520 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,520 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,521 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,521 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,523 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,530 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,532 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,532 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,532 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,532 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,532 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,532 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,532 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,533 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,533 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,534 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,535 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,535 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,535 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,535 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,535 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,536 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,538 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,541 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,541 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,543 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,543 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,543 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,544 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,544 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,544 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,544 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,544 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,544 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,544 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,544 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,544 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,544 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,544 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,546 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,546 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,546 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,547 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,548 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,548 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,548 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,548 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,548 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,549 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,549 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,551 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,552 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,553 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,553 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,553 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,553 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,553 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,553 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,555 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,556 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,557 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,557 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,558 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,559 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,560 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,567 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,568 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,568 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,570 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,571 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,571 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,571 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,571 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,571 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,571 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,572 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,573 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,573 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,574 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,574 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,574 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,574 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,574 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,574 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,575 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,577 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,578 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,579 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,579 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,580 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,580 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,580 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,580 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,581 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,581 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,582 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,585 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,586 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,592 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,594 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,603 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,603 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,604 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,604 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,608 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,610 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,611 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,611 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,611 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,611 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,611 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,611 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,613 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,616 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,617 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,630 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,631 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,637 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,639 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,640 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,642 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,643 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,643 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,643 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,643 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,643 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,643 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,644 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,644 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,645 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,645 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,645 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,649 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,649 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,650 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,651 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,651 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,651 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,651 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,651 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,651 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,652 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,653 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,656 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,658 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,659 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,661 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,662 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,662 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,662 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,662 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,662 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,662 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,663 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,669 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,670 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,671 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,671 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,671 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,671 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,672 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,672 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,674 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,683 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,684 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,691 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,692 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,713 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,713 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,714 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,715 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,715 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,715 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,715 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,715 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,715 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,715 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,716 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,716 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,716 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,717 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,717 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,717 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,718 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,718 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,718 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,718 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,718 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,719 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,719 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,721 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,723 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,725 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,725 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,726 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,726 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,726 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,726 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,726 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,728 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,740 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,741 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,755 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,756 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,768 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,769 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,770 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,770 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,771 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,771 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,771 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,771 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,773 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,784 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,786 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,787 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,787 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,787 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,787 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,787 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,787 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,788 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,789 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,790 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,791 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,791 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,791 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,792 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,792 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,792 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,793 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,794 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,794 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,804 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,805 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,830 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,832 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,833 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,833 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,833 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,833 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,833 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,833 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,833 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,834 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,835 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,837 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,837 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,838 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,838 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,839 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,839 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,839 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,839 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,839 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,839 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,840 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,841 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,841 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,848 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,849 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,850 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,850 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,851 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,851 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,851 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,851 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,853 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,858 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,860 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,860 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,861 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,861 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,861 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,861 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,861 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,861 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,861 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,862 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,868 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,869 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,870 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,871 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,871 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,871 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,871 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,871 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,872 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,872 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,872 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,872 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,872 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,872 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,872 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,872 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,872 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,873 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,873 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,873 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,873 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,873 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,873 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,873 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,874 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,874 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,874 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,874 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,876 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,880 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,882 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,882 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,883 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,883 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,883 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,883 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,883 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,885 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,888 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,889 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,902 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,903 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,904 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,904 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,904 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,904 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,905 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,906 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,907 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,907 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,916 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,917 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,918 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,918 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,918 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,918 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,918 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,918 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,919 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,928 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,929 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,929 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,930 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,931 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,931 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,931 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,931 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,932 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,932 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,932 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,932 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,932 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,932 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,933 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,933 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,933 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,933 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,933 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,933 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,933 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,934 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,934 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,934 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,934 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,934 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,934 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,934 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,934 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,934 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,934 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,934 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,934 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,934 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,934 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,935 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,935 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,935 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,936 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,937 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,945 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,946 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,949 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,950 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,951 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,953 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,953 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,954 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,959 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,960 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,960 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,961 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,961 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,961 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,961 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,961 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,961 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,961 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,962 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,962 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,964 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,965 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,965 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,965 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,965 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,966 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,966 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,968 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,982 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,983 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,983 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,984 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,984 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,984 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,984 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,984 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,985 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,988 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,989 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,990 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,990 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,990 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,990 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,991 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,991 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,991 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,991 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,992 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,996 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:26,997 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,998 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,999 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,999 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,999 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:26,999 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:26,999 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,001 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,001 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,002 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,002 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,003 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,010 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,011 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,014 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,015 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,016 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,016 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,016 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,016 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,016 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,016 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,017 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,021 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,021 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,022 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,022 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,022 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,022 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,022 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,022 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,022 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,023 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,023 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,025 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,027 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,028 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,028 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,028 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,028 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,028 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,028 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,028 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,029 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,030 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,030 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,030 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,030 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,030 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,031 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,031 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,033 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

Traceback (most recent call last):
  File "/root/Condenser/run_pre_training.py", line 219, in <module>
    main()
  File "/root/Condenser/run_pre_training.py", line 189, in main
    trainer.train(model_path=model_path)
  File "/root/con_venv/lib/python3.9/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
  File "/root/con_venv/lib/python3.9/site-packages/transformers/trainer.py", line 1928, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/root/con_venv/lib/python3.9/site-packages/accelerate/data_loader.py", line 452, in __iter__
    current_batch = next(dataloader_iter)
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/root/con_venv/lib/python3.9/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
AttributeError: Caught AttributeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/root/con_venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/root/Condenser/data.py", line 148, in __call__
    inputs, labels = self.mask_tokens(
AttributeError: 'CondenserCollator' object has no attribute 'mask_tokens'

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,039 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,040 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,042 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,043 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,057 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,058 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,068 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,068 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,069 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,069 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,070 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,071 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,095 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,096 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,096 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,097 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,098 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,098 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,098 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,098 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,099 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,099 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,099 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,100 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,101 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,103 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,104 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,105 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,107 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,108 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,108 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,108 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,108 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,108 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,109 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,110 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,110 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,111 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,112 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,112 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,112 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,112 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,112 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,113 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,114 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,114 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,116 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,117 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,117 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,117 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,117 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,117 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,117 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,117 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,118 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,119 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,135 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,137 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,138 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,138 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,138 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,138 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,138 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,139 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,140 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:27,140 >> Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,145 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,147 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,148 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,149 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,149 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,149 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,149 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,149 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,151 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,154 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,155 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,155 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,156 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,156 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,156 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,157 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,157 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,157 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,157 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,158 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,158 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,158 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,158 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,158 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,159 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,159 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,160 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,170 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,171 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,181 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,183 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,183 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,184 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,184 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,185 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,185 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,185 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,185 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,185 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,187 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,191 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,193 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,194 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,194 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,194 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,194 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,194 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,194 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,195 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,196 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,196 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,204 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,205 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,210 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,211 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,217 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,219 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,220 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,220 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,220 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,220 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,220 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,221 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,222 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,228 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,230 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,230 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,230 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,230 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,230 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,230 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,231 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,231 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,232 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,232 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,236 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,237 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,239 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,241 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,242 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,242 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,242 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,242 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,242 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,242 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,244 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,251 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,252 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,253 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,253 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,254 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,254 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,254 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,255 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,255 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,255 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,257 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,265 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,265 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,266 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,267 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,268 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,268 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,268 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,268 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,268 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,269 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,271 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,273 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,274 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,275 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,275 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,277 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,277 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,278 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,278 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,278 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,278 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,278 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,278 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,278 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,278 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,278 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,278 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,279 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,279 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,280 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,280 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,286 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,287 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,288 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,288 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,289 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,289 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,289 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,289 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,289 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,289 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,289 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,289 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,289 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,289 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,290 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,290 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,291 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,291 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,303 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,304 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,320 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,321 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,322 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,323 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,323 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,323 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,323 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,323 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,323 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,323 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,323 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,324 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,324 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,324 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,324 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,324 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,325 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,326 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,326 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,327 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,328 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,330 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,331 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,331 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,331 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,331 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,331 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,332 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,333 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,335 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,336 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,346 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,347 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,347 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,348 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,348 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,348 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,348 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,348 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,349 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,349 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,349 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,349 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,350 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,350 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,350 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,350 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,350 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,350 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,350 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,350 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,350 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,351 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,351 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,351 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,351 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,352 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,353 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,356 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,357 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,358 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,358 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,358 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,358 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,358 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,359 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,360 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,360 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,360 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,361 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,361 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,363 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,363 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,364 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,364 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,365 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,365 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,366 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,366 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,366 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,366 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,366 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,367 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,368 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,375 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,377 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,378 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,378 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,378 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,378 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,378 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,378 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,380 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,385 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,386 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,389 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,390 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,398 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,400 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,401 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,401 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,401 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,401 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,401 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,402 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,403 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,403 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,404 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,406 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,407 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,408 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,408 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,409 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,409 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,409 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,409 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,409 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,409 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,411 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,411 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,412 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,417 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,418 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,431 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,433 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,434 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,434 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,434 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,434 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,434 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,434 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,434 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,435 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,435 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,436 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,436 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,437 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,438 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,444 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,445 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,448 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,450 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,451 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,451 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,451 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,451 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,451 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,451 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,453 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,462 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,463 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,485 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,486 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,488 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,489 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,489 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,489 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,489 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,489 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,489 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,490 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,490 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,494 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,495 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,495 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,495 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,495 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,495 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,495 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,495 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,496 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,496 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,496 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,499 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,501 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,502 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,502 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,502 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,502 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,502 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,502 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,504 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,508 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,510 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,511 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,511 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,511 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,511 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,512 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,512 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,514 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,516 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,517 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,522 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,523 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,524 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,524 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,524 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,524 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,524 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,525 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,526 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,526 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,527 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,530 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,532 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,532 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,532 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,532 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,532 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,533 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,533 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,534 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,538 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,540 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,541 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:27,541 >> Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,541 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,541 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,541 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,541 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,541 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,543 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,557 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,559 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,560 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,560 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,560 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,560 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,561 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,561 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,563 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,563 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,564 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,565 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,565 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,565 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,566 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,566 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,566 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,566 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,566 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,568 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,575 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,577 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,577 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,577 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,577 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,577 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,577 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,578 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,578 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,578 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,579 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,587 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,588 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,604 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,605 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,610 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,612 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,613 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,613 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,614 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,614 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,614 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,614 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,614 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,615 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,616 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,617 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,618 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,619 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,619 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,620 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,620 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,620 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,620 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,620 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,622 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,622 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,623 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,624 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,632 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,634 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,635 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,635 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,635 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,635 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,635 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,635 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,637 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,638 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,638 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,638 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,640 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,640 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,641 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,641 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,641 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,641 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,641 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,642 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,643 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,643 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,644 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,646 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,647 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,647 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,647 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,647 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,647 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,647 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,648 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,649 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,649 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,649 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,650 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,650 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,650 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,650 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,650 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,651 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,651 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,653 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,653 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,653 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,654 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,654 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,654 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,654 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,655 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,663 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,664 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,664 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,665 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,665 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,665 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,665 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,665 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,665 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,666 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,666 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,666 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,667 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,667 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,667 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,667 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,667 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,669 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:27,671 >> Token indices sequence length is longer than the specified maximum sequence length for this model (944 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,682 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,684 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,684 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,684 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,686 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,686 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,686 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,686 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,687 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,687 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,687 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,687 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,689 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,694 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,695 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,696 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,696 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,698 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,698 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,699 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,699 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,699 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,699 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,699 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,699 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,699 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,699 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,699 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,699 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,699 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,700 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,701 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,701 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,703 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,705 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,706 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,706 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,706 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,706 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,706 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,707 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,708 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,710 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,712 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,712 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,713 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,713 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,713 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,713 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,713 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,715 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,719 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,721 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,722 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,723 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,723 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,723 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,723 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,723 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,724 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,724 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,725 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,725 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,726 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,734 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,735 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,738 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,738 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,739 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,740 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,746 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,746 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,747 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,747 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,747 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,748 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,748 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,748 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,747 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,748 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,748 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,749 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,749 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,749 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,749 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,749 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,749 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,750 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,750 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,752 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,752 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,752 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,754 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,754 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,755 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,755 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,755 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,755 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,755 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,755 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,756 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,756 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,757 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,761 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,763 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,763 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,764 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,764 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,764 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,764 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,764 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,766 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,769 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,770 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,771 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,772 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,773 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,773 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,773 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,774 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,774 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,774 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,776 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,784 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,785 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,799 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,801 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,802 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,804 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,811 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,814 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,815 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,815 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,815 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,815 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,815 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,816 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,816 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,817 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,817 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,842 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,843 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,844 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,844 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,844 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,844 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,845 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,845 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,845 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,845 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,846 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,847 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,847 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,847 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,847 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,847 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,847 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,847 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,848 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,849 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,852 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,853 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,855 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,856 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,856 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,856 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,857 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,857 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,857 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,857 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,858 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,864 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,865 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,872 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,873 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,878 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,879 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,883 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,885 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,886 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,886 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,886 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,886 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,886 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,886 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,886 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,887 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,887 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,888 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,888 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,888 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,888 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,888 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,888 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,888 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,888 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,888 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,888 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,888 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,888 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,889 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,889 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,890 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,891 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,898 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,900 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,900 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,901 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,901 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,901 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,901 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,901 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,901 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,903 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,903 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,904 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,904 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,904 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,904 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,904 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,906 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,907 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,907 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,908 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,908 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,908 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,908 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,908 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,908 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,909 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,909 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,909 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,909 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,909 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,909 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,910 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,910 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,911 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,918 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,919 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,926 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,928 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,932 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,933 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,948 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,950 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,951 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,951 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,951 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,951 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,951 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,952 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,953 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,959 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,959 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,961 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,961 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,962 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,962 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,962 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,963 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,963 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,963 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,963 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,964 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,965 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,979 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,980 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,981 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,982 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,991 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,993 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,994 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,994 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,994 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,994 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:27,994 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:27,995 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:27,996 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,002 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,002 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,003 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,004 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,004 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,004 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,004 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,004 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,004 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,005 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,005 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,006 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,006 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,006 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,006 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,006 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,006 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,008 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,009 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,010 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,013 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,014 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,021 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,022 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,023 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,025 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,026 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,026 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,026 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,026 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,026 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,027 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,028 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,031 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,031 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,032 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,033 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,033 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,033 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,034 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,034 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,034 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,034 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,035 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,037 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,039 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,040 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,040 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,041 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,041 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,041 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,041 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,041 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,043 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,043 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,052 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,054 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,055 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,055 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,055 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,055 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,055 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,056 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,057 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,058 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,058 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,059 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,059 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,059 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,059 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,059 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,059 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,060 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,060 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,060 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,061 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,061 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,061 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,061 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,061 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,062 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,063 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,065 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,067 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,068 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,068 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,068 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,068 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,068 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,068 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,068 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,069 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,070 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,073 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,075 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,076 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,076 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,076 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,076 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,076 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,077 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,078 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,081 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,082 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,082 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,084 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,084 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,085 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,085 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,085 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,085 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,085 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,085 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,086 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,087 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,095 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,096 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,096 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,096 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,097 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,098 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,099 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,099 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,099 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,099 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,099 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,099 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,101 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,113 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,115 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,116 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,116 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,116 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,117 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,117 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,117 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,119 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,120 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,121 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,122 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,123 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,130 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,132 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,133 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,133 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,133 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,133 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,133 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,133 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,135 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,142 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,142 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,143 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,144 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,144 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,144 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,145 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,145 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,145 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,145 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,145 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,145 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,146 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,146 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,147 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,147 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,147 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,147 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,147 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,147 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,147 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,147 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,148 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,148 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,148 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,149 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,149 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,150 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,151 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,151 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,151 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,151 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,151 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,151 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,152 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,156 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,157 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,179 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,180 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,190 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,192 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,198 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,199 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,199 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,200 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,200 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,201 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,201 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,201 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,201 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,201 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,202 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,202 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,202 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,202 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,202 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,202 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,203 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,204 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,204 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,205 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,213 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,214 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,215 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,215 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,216 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,216 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,216 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,216 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,218 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,218 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,219 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,227 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,228 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,232 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,233 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,233 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,234 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,242 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,243 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,243 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,243 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,243 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,243 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,243 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,244 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,244 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,245 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,245 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,246 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,247 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,247 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,247 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,247 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,247 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,247 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,247 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,247 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,248 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,248 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,248 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,248 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,248 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,249 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,249 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,249 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,249 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,249 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,249 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,250 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,250 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,250 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,251 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,252 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:28,261 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1736 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,271 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,272 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:28,276 >> Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,276 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,278 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,279 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,279 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,279 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,279 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,279 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,279 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,281 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,285 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,287 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,293 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,293 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,294 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,295 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,295 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,296 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,296 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,296 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,296 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,296 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,297 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,297 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,298 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,298 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,298 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,298 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,298 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,298 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,298 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,299 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,299 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,300 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,300 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,300 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,301 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,301 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,301 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,301 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,303 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,308 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,309 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,319 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,320 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,322 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,323 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,329 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,330 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,342 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,343 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,344 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,345 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,345 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,345 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,345 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,345 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,345 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,345 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,346 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,346 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,346 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,346 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,346 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,346 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,347 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,348 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,349 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,350 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,358 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,359 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,360 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,360 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,360 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,360 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,361 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,361 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,361 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,363 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,363 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,364 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,364 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,364 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,364 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,364 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,365 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,365 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,366 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,366 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,369 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,369 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,369 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,370 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,371 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,371 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,371 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,371 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,372 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,372 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,372 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,372 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,372 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,372 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,372 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,372 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,372 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,372 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,372 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,373 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,374 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,374 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,379 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,380 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,381 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,381 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,381 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,382 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,382 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,382 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,384 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,386 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[2024-03-18 06:41:28,387] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 380508 closing signal SIGTERM
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,389 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,390 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,390 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,390 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,390 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,390 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,390 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,390 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,392 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,392 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,393 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,393 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,393 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,393 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,393 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,394 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,395 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,404 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,406 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,407 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,407 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,407 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,407 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,407 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,408 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,409 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,413 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,414 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,415 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,415 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,415 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,415 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,416 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,416 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,418 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,423 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,424 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,425 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,425 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,426 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,426 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,426 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,426 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,426 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,426 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,426 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,426 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,426 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,426 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,426 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,427 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,427 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,428 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,430 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,431 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,440 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,440 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,441 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,441 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,441 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,442 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,444 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,444 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,446 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,446 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,447 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,447 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,447 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,447 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,447 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,447 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,447 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,447 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,447 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,447 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,448 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,448 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,449 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,449 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,455 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,455 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,456 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,456 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,460 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,461 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,473 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,474 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,477 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,478 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,481 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,482 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,482 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,482 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,483 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,483 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,483 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,483 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,483 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,483 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,484 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,484 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,485 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,485 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,485 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,485 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,485 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,485 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,486 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,488 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,489 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,489 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,490 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,491 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,492 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,492 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,492 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,492 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,492 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,493 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,494 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[2024-03-18 06:41:28,504] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 380509) of binary: /root/con_venv/bin/python3.9
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,505 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,506 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,507 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,507 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,507 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,507 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,508 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,508 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,508 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,508 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

Traceback (most recent call last):
  File "/root/con_venv/bin/torchrun", line 8, in <module>
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,510 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

    sys.exit(main())
  File "/root/con_venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/root/con_venv/lib/python3.9/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/root/con_venv/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/root/con_venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/con_venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,511 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_pre_training.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-18_06:41:28
  host      : ab6613b10048
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 380510)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-18_06:41:28
  host      : ab6613b10048
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 380511)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-18_06:41:28
  host      : ab6613b10048
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 380509)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,512 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,515 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,517 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,518 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,518 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,518 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,518 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,518 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,518 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,520 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,523 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,525 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,526 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,526 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,526 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,526 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,526 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,526 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,528 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,529 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,531 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,532 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,532 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,532 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,532 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,532 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,533 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,534 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,535 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,536 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,547 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,548 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,566 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,568 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,569 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,569 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,569 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,569 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,569 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,570 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,571 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,573 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,574 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,575 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,576 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,577 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,578 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,578 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,578 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,578 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,578 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,578 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,579 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,578 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,579 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,579 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,579 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,579 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,579 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,580 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,580 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,580 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,580 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,581 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,581 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,590 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,591 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,594 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,595 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,597 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,599 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,600 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,600 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,600 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,600 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,601 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,601 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,603 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,605 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,607 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,608 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,608 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,608 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,608 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,608 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,608 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,609 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,610 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,610 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,621 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,622 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,627 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,629 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,630 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,630 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,630 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,630 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,631 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,631 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,633 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,642 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,644 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,644 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,645 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,646 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,646 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,646 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,646 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,646 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,646 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,646 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,646 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,647 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,647 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,648 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,650 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,651 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,651 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,651 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,651 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,651 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,653 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,654 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,655 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,682 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,683 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,693 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,694 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,694 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,696 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,697 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,697 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,697 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,697 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,697 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,697 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,697 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,698 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,699 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,719 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,720 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,722 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,723 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,727 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,729 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,730 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,730 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,730 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,730 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,730 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,730 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,731 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,732 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,732 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,733 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,733 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,733 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,733 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,733 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,733 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,735 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,738 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,740 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,741 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,741 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,741 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,741 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,741 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,742 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,743 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,743 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,747 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,747 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,748 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,748 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,748 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,748 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,748 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,749 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,749 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,749 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,749 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,749 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,749 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,749 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,749 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,750 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,750 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,754 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,755 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,756 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,757 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,757 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,757 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,757 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,757 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,757 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,757 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,757 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,757 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,757 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,757 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,757 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,758 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:28,758 >> Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,759 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,759 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,759 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,761 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,762 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,762 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,762 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,762 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,762 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,763 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,764 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,766 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,767 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,768 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,770 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,770 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,771 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,771 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,771 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,771 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,771 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,773 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,780 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,781 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,782 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,782 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,782 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,782 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,782 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,782 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,783 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,783 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,783 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,784 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,784 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,784 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,784 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,784 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,784 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,786 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,812 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,813 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,816 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,817 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,817 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,819 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,820 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,820 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,820 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,820 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,820 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,821 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,822 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,833 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,833 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,834 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,834 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,837 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,839 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,842 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,843 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,844 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,844 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,845 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,845 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,845 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,845 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,846 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,847 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,847 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,848 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,848 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,848 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,848 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,848 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,849 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,850 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,850 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,852 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,853 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,853 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,853 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,853 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,853 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,853 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,855 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,858 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,859 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,860 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,861 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,862 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,862 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,863 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,863 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,863 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,863 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,864 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,864 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,864 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,865 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,866 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,866 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,866 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,866 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,866 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,866 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,867 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,868 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,869 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,871 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,873 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,874 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,880 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,881 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,882 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,882 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,883 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,883 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,883 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,883 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,883 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,884 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,885 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,887 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,887 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,889 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,889 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,890 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,890 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,890 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,890 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,890 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,890 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,890 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,890 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,890 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,890 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,891 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,891 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,892 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,893 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,898 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,899 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,899 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,900 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,907 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,908 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,908 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,908 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,908 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,908 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,908 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,909 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,910 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,910 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,910 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,910 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,910 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,911 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,911 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,911 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,911 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,912 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,913 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,914 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,914 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,914 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,914 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,914 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,915 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,916 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,931 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,932 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,932 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,933 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,933 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,933 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,933 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,933 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,933 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,934 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,935 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,942 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,943 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,950 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,951 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,952 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,953 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,953 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,954 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,955 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,955 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,956 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,956 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,956 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,956 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,956 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,957 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,958 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,965 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,966 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,974 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,975 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,976 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,977 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,977 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,978 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,978 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,978 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,978 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,978 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,978 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,979 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,979 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,979 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,979 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,979 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,981 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,983 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,984 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,985 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,985 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,985 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,985 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:28,985 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:28,985 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:28,986 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,001 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,003 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,004 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,004 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,004 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,004 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,004 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,004 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,006 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,017 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,018 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,019 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,019 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,019 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,019 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,020 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,020 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,020 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,020 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,020 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,020 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,021 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,021 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,021 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,021 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,022 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,022 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,022 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,024 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,026 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,027 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,029 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,030 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

wandb: - 0.028 MB of 0.037 MB uploaded[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,043 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,045 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,046 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,046 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,046 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,046 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,046 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,046 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,047 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,070 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,071 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,071 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,072 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,072 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,072 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,072 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,072 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,072 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,072 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,073 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,074 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,075 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,081 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,082 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,094 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,095 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,103 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,104 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,125 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,126 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,136 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,138 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,138 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,139 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,139 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,139 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,139 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,139 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,140 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,140 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,140 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,140 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,140 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,140 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,140 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,140 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,140 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,141 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,141 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,141 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,141 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,141 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,141 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,141 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,142 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,142 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,142 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,142 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,143 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,143 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,144 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,144 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,144 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,144 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,145 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,145 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,145 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,145 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,145 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,145 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,145 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,145 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,145 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,146 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,147 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,151 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,152 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,152 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,153 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,153 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,153 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,153 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,153 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,153 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,154 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,155 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,155 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,155 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,155 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,155 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,155 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,156 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,157 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,158 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,159 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,161 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,163 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,165 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,164 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,165 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,165 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,165 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,165 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,165 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,166 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,167 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,167 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,167 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,167 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,167 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,168 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,168 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,170 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,171 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,173 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,174 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,174 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,174 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,174 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,174 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,175 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,177 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,181 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,183 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,184 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,184 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,184 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,184 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,184 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,185 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,186 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,206 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,207 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,211 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,213 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,216 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,217 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,217 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,218 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,218 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,220 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,221 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,221 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,221 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,221 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,221 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,221 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,223 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,223 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,224 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,224 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,225 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,226 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,227 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,228 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,228 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,228 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,229 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,229 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,229 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,230 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,231 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,232 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,233 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,233 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,233 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,233 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,233 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,233 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,235 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,236 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,237 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,239 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,239 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,241 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,241 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,241 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,241 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,241 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,241 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,242 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,243 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,246 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,247 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,247 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,247 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,247 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,247 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,247 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,248 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,248 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,250 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,250 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,251 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,251 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,251 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,251 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,251 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,251 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,252 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,252 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,252 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,253 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,253 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,253 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,253 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,254 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,254 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,254 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,254 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,255 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,256 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,258 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,260 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,267 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,268 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,269 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,269 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,269 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,269 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,269 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,269 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,270 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,281 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,282 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,286 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,288 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,289 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,289 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,290 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,290 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,290 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,290 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,292 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,292 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,293 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,303 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,304 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,305 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,306 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,307 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,308 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,308 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,308 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,308 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,308 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,308 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,308 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,309 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,309 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,309 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,309 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,309 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,309 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,310 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,310 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,314 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,315 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,316 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,317 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,318 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,318 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,326 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,327 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,327 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,328 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,335 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,336 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,336 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,337 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,337 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,337 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,337 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,338 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,338 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,338 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,339 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,339 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,339 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,339 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,339 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,339 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,339 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,341 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,356 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,357 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,358 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,358 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,358 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,358 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,358 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,358 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,359 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,363 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,365 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,366 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,366 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,366 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,366 >> loading file special_tokens_map.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,366 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,366 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,367 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,367 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,368 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,368 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,368 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,368 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,368 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,368 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,369 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,370 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,372 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,373 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,373 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,374 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,375 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,376 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,377 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,377 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,378 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,378 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,378 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,378 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,380 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,393 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,394 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,394 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,395 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,395 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,395 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,396 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,396 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,396 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,396 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,398 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,400 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,401 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,407 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,408 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,420 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,422 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,423 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,423 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,423 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,423 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,423 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,423 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,425 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,439 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,440 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,441 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,443 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,444 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,444 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,444 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,444 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,444 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,444 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,444 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,445 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,446 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,450 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,451 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,460 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,462 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,466 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,467 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,467 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,469 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,470 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,470 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,470 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,470 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,470 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,471 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,471 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,472 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,472 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,482 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,482 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,484 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,484 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,484 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,485 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,485 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,485 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,485 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,485 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,485 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,485 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,485 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,485 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,485 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,486 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,487 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,487 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,491 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,492 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,510 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,511 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,511 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,511 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,511 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,512 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,512 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,512 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,513 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,523 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,525 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,525 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,526 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,526 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,526 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,526 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,526 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,528 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,528 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,529 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,535 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,536 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,536 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,536 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,536 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,536 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,536 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,537 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,537 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,537 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,539 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,540 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,540 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,540 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,540 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,540 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,541 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,542 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,547 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,548 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,548 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,550 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,550 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,551 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,551 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,551 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,551 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,551 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,553 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,557 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,558 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,564 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,565 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,570 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,572 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,573 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,573 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,573 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,573 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,573 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,574 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,575 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,587 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,587 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,588 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,589 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,589 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,589 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,590 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,590 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,590 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,590 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,590 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,590 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,591 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,591 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,591 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,592 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,592 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,592 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,592 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,592 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,592 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,592 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,592 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,592 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,592 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,592 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,592 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,592 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,593 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,593 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,593 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,593 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,593 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,593 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,593 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,594 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,594 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,595 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,595 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,596 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,597 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,597 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,597 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,597 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,598 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,598 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,600 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,604 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,605 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,606 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,607 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,607 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,607 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,607 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,607 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,608 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,608 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,607 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,608 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,608 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,608 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,608 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,608 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,608 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,608 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,609 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,610 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,610 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,610 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,610 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,611 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,617 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,618 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,621 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,623 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,624 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,624 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,632 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,632 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,632 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,633 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,634 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,645 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,646 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,662 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,662 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,663 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,663 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,668 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,669 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,671 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,671 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,672 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,673 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,673 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,674 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,674 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,674 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,674 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,674 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,674 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,674 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,674 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,674 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,674 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,674 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,675 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,676 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,676 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,677 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,684 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,686 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,686 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,686 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,687 >> loading file added_tokens.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,686 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,687 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,687 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,687 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,688 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,689 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,689 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,690 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,698 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:29,699 >> Token indices sequence length is longer than the specified maximum sequence length for this model (856 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,699 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,705 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,707 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,708 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,708 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,708 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,708 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,708 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,709 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,710 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,711 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,711 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,712 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,713 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,713 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,714 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,714 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,714 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,714 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,714 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,715 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:29,728 >> Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,739 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,741 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,741 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,741 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,741 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,741 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,741 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,742 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,742 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,743 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,744 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,746 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,747 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,754 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,756 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,756 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,757 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,757 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,757 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,757 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,757 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,758 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,758 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,758 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,758 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,759 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,759 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,759 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,759 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,759 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,759 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,759 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,760 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,760 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,760 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,761 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,761 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,761 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,761 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,763 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,783 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,783 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,784 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,785 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,785 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,785 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,786 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,786 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,786 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,786 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,786 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,786 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,787 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,787 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,787 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,787 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,787 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,787 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,788 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,788 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,788 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,789 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,789 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,789 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,789 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,789 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,789 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,790 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,790 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,800 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,801 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,808 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,809 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,810 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,810 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,811 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,811 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,812 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,812 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,812 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,813 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,813 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,813 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,813 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,814 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,814 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,815 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,815 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,815 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,815 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,815 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,815 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,821 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,827 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,828 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,836 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,837 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,838 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,840 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,840 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,840 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,840 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,840 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,840 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,841 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,842 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:29,844 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1381 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,845 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,846 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,856 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,857 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,858 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,858 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,859 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,859 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,859 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,859 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,859 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,859 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,861 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,868 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,869 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,876 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,878 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,879 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,879 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,879 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,879 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,879 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,879 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,880 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,880 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,881 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,881 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,881 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,881 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,882 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,882 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,882 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,882 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,882 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,882 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,883 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,883 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,883 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,883 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,883 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,884 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,885 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,887 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,889 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,890 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,890 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,890 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,890 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,890 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,890 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,891 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,891 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,892 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,893 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,894 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,895 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,895 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,895 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,895 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,895 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,896 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,896 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,896 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,897 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,910 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,912 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,931 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,933 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,934 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,934 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,934 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,934 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,934 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,935 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,936 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,946 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,946 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,948 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,948 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,949 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,949 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,949 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,949 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,949 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,949 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,949 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,950 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,950 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,950 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,951 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,952 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,953 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,954 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,955 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,956 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,958 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,959 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,959 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,959 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,959 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,959 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,960 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,961 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,961 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,963 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,963 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,964 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,964 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,964 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,964 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,964 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,966 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,970 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,971 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,972 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,973 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,974 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,974 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,974 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,975 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,975 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,975 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,977 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,982 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,983 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:29,985 >> Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,988 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,989 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,990 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,990 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,991 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,991 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:29,991 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:29,991 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:29,993 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,007 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,008 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,013 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,015 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,016 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,016 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,016 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,016 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,016 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,016 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,018 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,021 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,021 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,022 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,023 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,024 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,024 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,024 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,024 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,024 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,025 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,026 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,029 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,030 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,039 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,040 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

wandb: \ 0.028 MB of 0.037 MB uploaded[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,053 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,054 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,056 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,058 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,059 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,059 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,059 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,059 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,059 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,059 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,060 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,061 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,061 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,061 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,061 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,062 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,062 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,062 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,062 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,063 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,069 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,070 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,076 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
wandb: | 0.037 MB of 0.037 MB uploaded[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,078 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,079 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,079 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,079 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,079 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,079 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,080 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,081 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,094 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,095 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,102 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,103 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,110 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,111 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,112 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,112 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,113 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,113 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,113 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,113 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,115 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,119 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,122 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,123 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,123 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,123 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,123 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,123 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,124 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,126 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,128 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,129 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,137 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,138 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,140 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,142 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,142 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,142 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,142 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,142 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,142 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,143 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,144 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,148 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,149 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,168 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,169 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,169 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,170 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,170 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,170 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,170 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,170 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,170 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,170 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,171 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,171 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,171 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,172 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,172 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,172 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,172 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,172 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,173 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,173 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,173 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,173 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,173 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,173 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,173 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,174 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,175 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,175 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,176 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,177 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,177 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,177 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,177 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,177 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,177 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,179 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,180 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,182 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,182 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,182 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,182 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,182 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,182 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,183 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,184 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,184 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,185 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,193 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,194 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,195 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,195 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,195 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,195 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,195 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,196 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,197 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,211 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,213 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,214 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,214 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,214 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,214 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,214 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,215 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,216 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,217 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,219 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,220 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,220 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,220 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,220 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,220 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,220 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,221 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,222 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,223 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,226 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,228 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,229 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,229 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,229 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,229 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,229 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,230 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,231 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,234 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,235 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,236 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,236 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,237 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,237 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,237 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,237 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,238 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,238 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,238 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,238 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,238 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,238 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,238 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,238 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,240 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,240 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,252 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,254 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,254 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,255 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,255 >> loading file tokenizer.json from cache at None
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,255 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,255 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,255 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,255 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,256 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,256 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,256 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,257 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,257 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,257 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,257 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,257 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,257 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

wandb: üöÄ View run wobbly-totem-9 at: https://wandb.ai/3jp4rk/condenser-bert-pretrain/runs/loax5ril
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,258 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
wandb: Find logs at: ./wandb/run-20240318_064118-loax5ril/logs
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,259 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,259 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,260 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,267 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,268 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,282 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,283 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,298 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,300 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,301 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,301 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,302 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,302 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,302 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,304 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,311 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,312 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,314 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,314 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,316 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,316 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,317 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,317 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,317 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,317 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,317 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,317 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,318 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,324 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,325 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,337 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,338 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,340 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,341 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,341 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,341 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,341 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,341 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,343 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,343 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,344 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,361 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,362 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,370 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,372 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,373 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,373 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,373 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,374 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,374 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,374 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,375 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,375 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,376 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,403 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,405 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,406 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,406 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,406 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,406 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,407 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,407 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,409 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,422 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,423 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,429 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,430 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,431 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,431 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,431 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,431 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,431 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,431 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,432 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,444 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,445 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,446 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,446 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[WARNING|tokenization_utils_base.py:3867] 2024-03-18 06:41:30,489 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1752 > 512). Running this sequence through the model will result in indexing errors
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,491 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,493 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,494 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,494 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,494 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,494 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,494 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,494 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,495 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,496 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,496 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,520 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,521 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,558 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,560 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,562 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,562 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,562 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,562 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,562 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,563 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,565 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,568 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,569 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,573 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,575 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,575 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,576 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,576 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,576 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,576 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,576 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,578 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,591 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,593 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,594 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,594 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,594 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,594 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,594 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,594 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,595 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,595 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,596 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,596 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,596 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,596 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,596 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,596 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,596 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,597 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,602 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,604 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,605 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,605 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,605 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,605 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,606 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,606 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,608 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,610 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,611 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,612 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,612 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,612 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,612 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,612 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,612 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,613 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,646 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,647 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,661 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,662 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,665 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,666 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,679 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,681 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,685 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,686 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,701 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,702 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,863 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,870 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,871 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,872 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,872 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,872 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,872 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,872 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,874 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,937 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,937 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,938 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,939 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,939 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,939 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,939 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,940 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,940 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,940 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/vocab.txt
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,940 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,940 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,940 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,940 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-03-18 06:41:30,940 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/tokenizer_config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,941 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:728] 2024-03-18 06:41:30,941 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,942 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,942 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:791] 2024-03-18 06:41:30,942 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:31,006 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:31,007 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|configuration_utils.py:728] 2024-03-18 06:41:31,008 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--tunib--electra-ko-en-base/snapshots/8004b116b7cac4b0ade59d1da0e58641da725788/config.json
[INFO|configuration_utils.py:791] 2024-03-18 06:41:31,009 >> Model config ElectraConfig {
  "_name_or_path": "tunib/electra-ko-en-base",
  "architectures": [
    "ElectraForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "embedding_size": 768,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "electra",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "summary_activation": "gelu",
  "summary_last_dropout": 0.1,
  "summary_type": "first",
  "summary_use_proj": true,
  "transformers_version": "4.38.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

