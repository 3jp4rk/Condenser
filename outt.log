[2024-03-30 19:58:37,567] torch.distributed.run: [WARNING] 
[2024-03-30 19:58:37,567] torch.distributed.run: [WARNING] *****************************************
[2024-03-30 19:58:37,567] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-30 19:58:37,567] torch.distributed.run: [WARNING] *****************************************
wandb: Currently logged in as: 3jp4rk. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: 3jp4rk. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /home/ubuntu/ejpark/Condenser/wandb/run-20240330_195841-ijb65upm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-bee-38
wandb: ‚≠êÔ∏è View project at https://wandb.ai/3jp4rk/Condenser
wandb: üöÄ View run at https://wandb.ai/3jp4rk/Condenser/runs/ijb65upm/workspace
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /home/ubuntu/ejpark/Condenser/wandb/run-20240330_195841-t809lagv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-elevator-37
wandb: ‚≠êÔ∏è View project at https://wandb.ai/3jp4rk/Condenser
wandb: üöÄ View run at https://wandb.ai/3jp4rk/Condenser/runs/t809lagv/workspace
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
03/30/2024 19:59:22 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
03/30/2024 19:59:22 - INFO - __main__ -   Training/evaluation parameters CoCondenserPreTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
cache_chunk_size=-1,
data_seed=None,
dataloader_drop_last=True,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=5000.0,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=co_condenser_pretrain/runs/Mar30_19-58-47_gpu-1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=8.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=co_condenser_pretrain,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=co_condenser_pretrain,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=20000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.1,
warmup_steps=0,
weight_decay=0.01,
)
[INFO|configuration_utils.py:724] 2024-03-30 19:59:22,967 >> loading configuration file /home/ubuntu/ejpark/checkpoint-260000/config.json
[INFO|configuration_utils.py:789] 2024-03-30 19:59:22,969 >> Model config BertConfig {
  "_name_or_path": "/home/ubuntu/ejpark/checkpoint-260000/",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.39.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 61790
}

[INFO|tokenization_utils_base.py:2082] 2024-03-30 19:59:22,970 >> loading file vocab.txt
[INFO|tokenization_utils_base.py:2082] 2024-03-30 19:59:22,970 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2082] 2024-03-30 19:59:22,970 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2082] 2024-03-30 19:59:22,970 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2082] 2024-03-30 19:59:22,970 >> loading file tokenizer.json
[INFO|modeling_utils.py:3280] 2024-03-30 19:59:23,049 >> loading weights file /home/ubuntu/ejpark/checkpoint-260000/model.safetensors
[INFO|configuration_utils.py:928] 2024-03-30 19:59:23,059 >> Generate config GenerationConfig {
  "pad_token_id": 0
}

[INFO|modeling_utils.py:4024] 2024-03-30 19:59:23,698 >> All model checkpoint weights were used when initializing BertForMaskedLM.

[INFO|modeling_utils.py:4032] 2024-03-30 19:59:23,698 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at /home/ubuntu/ejpark/checkpoint-260000/.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
[INFO|configuration_utils.py:881] 2024-03-30 19:59:23,701 >> loading configuration file /home/ubuntu/ejpark/checkpoint-260000/generation_config.json
[INFO|configuration_utils.py:928] 2024-03-30 19:59:23,701 >> Generate config GenerationConfig {
  "pad_token_id": 0
}

03/30/2024 19:59:23 - INFO - modeling -   loading extra weights from local files
-------------------------------------------
loading json . . .
03/30/2024 19:59:43 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
-------------------------------------------
loading json . . .
/home/ubuntu/ejpark/new_index_diet__.json load complete! 482.6542203426361 seconds took.
03/30/2024 20:07:26 - INFO - trainer -   Initializing Gradient Cache Trainer
/home/ubuntu/ejpark/con_venv/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/ubuntu/ejpark/new_index_diet__.json load complete! 482.07337951660156 seconds took.
/home/ubuntu/ejpark/con_venv/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:1969] 2024-03-30 20:09:28,470 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-03-30 20:09:28,470 >>   Num examples = 7,204
[INFO|trainer.py:1971] 2024-03-30 20:09:28,470 >>   Num Epochs = 8
[INFO|trainer.py:1972] 2024-03-30 20:09:28,471 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1975] 2024-03-30 20:09:28,471 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1976] 2024-03-30 20:09:28,471 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1977] 2024-03-30 20:09:28,471 >>   Total optimization steps = 1,800
[INFO|trainer.py:1978] 2024-03-30 20:09:28,472 >>   Number of trainable parameters = 147,735,134
[INFO|integration_utils.py:723] 2024-03-30 20:09:28,473 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|          | 0/1800 [00:00<?, ?it/s]/home/ubuntu/ejpark/con_venv/lib/python3.9/site-packages/transformers/modeling_utils.py:977: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/home/ubuntu/ejpark/con_venv/lib/python3.9/site-packages/transformers/modeling_utils.py:977: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
loss: nan, co: nan
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
loss: nan, co: nan
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/1800 [06:08<184:21:26, 368.92s/it]                                                     {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 5.555555555555556e-07, 'epoch': 0.0}
  0%|          | 1/1800 [06:08<184:21:26, 368.92s/it]loss: nan, co: nan
loss: nan, co: nan
  0%|          | 2/1800 [06:10<76:23:29, 152.95s/it]                                                     {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 1.1111111111111112e-06, 'epoch': 0.01}
  0%|          | 2/1800 [06:10<76:23:29, 152.95s/it]loss: nan, co: nanloss: nan, co: nan

  0%|          | 3/1800 [06:12<41:47:24, 83.72s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.01}
  0%|          | 3/1800 [06:12<41:47:24, 83.72s/it]loss: nan, co: nanloss: nan, co: nan

  0%|          | 4/1800 [08:39<54:18:18, 108.85s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.2222222222222225e-06, 'epoch': 0.02}
  0%|          | 4/1800 [08:39<54:18:18, 108.85s/it]loss: nan, co: nanloss: nan, co: nan

  0%|          | 5/1800 [08:40<34:56:32, 70.08s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.777777777777778e-06, 'epoch': 0.02}
  0%|          | 5/1800 [08:40<34:56:32, 70.08s/it]loss: nan, co: nanloss: nan, co: nan

  0%|          | 6/1800 [08:42<23:16:29, 46.71s/it]                                                   {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.03}
  0%|          | 6/1800 [08:42<23:16:29, 46.71s/it]loss: nan, co: nanloss: nan, co: nan

  0%|          | 7/1800 [08:43<15:52:27, 31.87s/it]                                                   {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 3.888888888888889e-06, 'epoch': 0.03}
  0%|          | 7/1800 [08:43<15:52:27, 31.87s/it]loss: nan, co: nanloss: nan, co: nan

  0%|          | 8/1800 [12:44<49:05:22, 98.62s/it]                                                   {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 4.444444444444445e-06, 'epoch': 0.04}
  0%|          | 8/1800 [12:44<49:05:22, 98.62s/it]loss: nan, co: nan
loss: nan, co: nan
  0%|          | 9/1800 [12:46<33:55:55, 68.20s/it]                                                   {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 5e-06, 'epoch': 0.04}
  0%|          | 9/1800 [12:46<33:55:55, 68.20s/it]loss: nan, co: nanloss: nan, co: nan

  1%|          | 10/1800 [12:47<23:38:53, 47.56s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 5.555555555555556e-06, 'epoch': 0.04}
  1%|          | 10/1800 [12:47<23:38:53, 47.56s/it]loss: nan, co: nan
loss: nan, co: nan
  1%|          | 11/1800 [12:48<16:36:15, 33.41s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 6.111111111111111e-06, 'epoch': 0.05}
  1%|          | 11/1800 [12:48<16:36:15, 33.41s/it]loss: nan, co: nanloss: nan, co: nan

  1%|          | 12/1800 [17:28<53:47:57, 108.32s/it]                                                     {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.05}
  1%|          | 12/1800 [17:28<53:47:57, 108.32s/it]loss: nan, co: nanloss: nan, co: nan

  1%|          | 13/1800 [17:29<37:40:51, 75.91s/it]                                                     {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.222222222222222e-06, 'epoch': 0.06}
  1%|          | 13/1800 [17:29<37:40:51, 75.91s/it]loss: nan, co: nanloss: nan, co: nan

  1%|          | 14/1800 [17:31<26:29:03, 53.38s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.777777777777777e-06, 'epoch': 0.06}
  1%|          | 14/1800 [17:31<26:29:03, 53.38s/it]loss: nan, co: nanloss: nan, co: nan

  1%|          | 15/1800 [17:32<18:41:23, 37.69s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.07}
  1%|          | 15/1800 [17:32<18:41:23, 37.69s/it]loss: nan, co: nan
loss: nan, co: nan
  1%|          | 16/1800 [20:47<42:09:51, 85.09s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 8.88888888888889e-06, 'epoch': 0.07}
  1%|          | 16/1800 [20:47<42:09:51, 85.09s/it]loss: nan, co: nan
loss: nan, co: nan
  1%|          | 17/1800 [20:49<29:40:04, 59.90s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 9.444444444444445e-06, 'epoch': 0.08}
  1%|          | 17/1800 [20:49<29:40:04, 59.90s/it]loss: nan, co: nanloss: nan, co: nan

  1%|          | 18/1800 [20:50<20:56:22, 42.30s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 1e-05, 'epoch': 0.08}
  1%|          | 18/1800 [20:50<20:56:22, 42.30s/it]loss: nan, co: nan
loss: nan, co: nan
  1%|          | 19/1800 [20:51<14:50:25, 30.00s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 1.0555555555555555e-05, 'epoch': 0.08}
  1%|          | 19/1800 [20:51<14:50:25, 30.00s/it]loss: nan, co: nan
loss: nan, co: nan
  1%|          | 20/1800 [26:18<58:52:08, 119.06s/it]                                                     {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.09}
  1%|          | 20/1800 [26:18<58:52:08, 119.06s/it]loss: nan, co: nanloss: nan, co: nan

  1%|          | 21/1800 [26:19<41:22:20, 83.72s/it]                                                     {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.09}
  1%|          | 21/1800 [26:19<41:22:20, 83.72s/it]loss: nan, co: nanloss: nan, co: nan

  1%|          | 22/1800 [26:21<29:08:11, 58.99s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 1.2222222222222222e-05, 'epoch': 0.1}
  1%|          | 22/1800 [26:21<29:08:11, 58.99s/it]loss: nan, co: nanloss: nan, co: nan

  1%|‚ñè         | 23/1800 [26:22<20:34:44, 41.69s/it]                                                    {'loss': 0.0, 'grad_norm': nan, 'learning_rate': 1.2777777777777777e-05, 'epoch': 0.1}
  1%|‚ñè         | 23/1800 [26:22<20:34:44, 41.69s/it][2024-03-30 20:36:24,687] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -15) local_rank: 0 (pid: 3214640) of binary: /home/ubuntu/ejpark/con_venv/bin/python3.9
Traceback (most recent call last):
  File "/home/ubuntu/ejpark/con_venv/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/ubuntu/ejpark/con_venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/ejpark/con_venv/lib/python3.9/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/home/ubuntu/ejpark/con_venv/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/home/ubuntu/ejpark/con_venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/ejpark/con_venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=========================================================
run_co_pre_training.py FAILED
---------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-30_20:36:24
  host      : gpu-1.novalocal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 3214641)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 3214641
---------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-30_20:36:24
  host      : gpu-1.novalocal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 3214640)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 3214640
=========================================================
